{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_io as tfio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "          file_contents,\n",
    "          desired_channels=1)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 22:00:20.435203: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-14 22:00:20.498085: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-07-14 22:00:20.498789: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-14 22:00:20.499545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 22:00:38.041624: I tensorflow_io/core/kernels/cpu_check.cc:128] Your CPU supports instructions that this TensorFlow IO binary was not compiled to use: AVX2 FMA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    }
   ],
   "source": [
    "testing_wav_data = load_wav_16k_mono(\"Downloads/ml-stuttering-events-dataset/clips/FluencyBank/178/FluencyBank_178_8.wav\")   \n",
    "\n",
    "pd_data = pd.read_csv(\"yamnet_data.csv\")\n",
    "\n",
    "my_classes = ['Humming', 'Speech']\n",
    "\n",
    "filenames = pd_data['filename']\n",
    "targets = pd_data['target']\n",
    "folds = pd_data['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
     ]
    }
   ],
   "source": [
    "def load_wav_for_map(filename, label, fold):\n",
    "  return load_wav_16k_mono(filename), label, fold\n",
    "\n",
    "main_ds = main_ds.map(load_wav_for_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embedding(wav_data, label, fold):\n",
    "  ''' run YAMNet to extract embedding from the wav data '''\n",
    "  scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "  num_embeddings = tf.shape(embeddings)[0]\n",
    "  return (embeddings,\n",
    "            tf.repeat(label, num_embeddings),\n",
    "            tf.repeat(fold, num_embeddings))\n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 89)\n",
    "val_ds = cached_ds.filter(lambda embedding, label, fold: 89 <= fold and fold <= 97)\n",
    "test_ds = cached_ds.filter(lambda embedding, label, fold: 98 <= fold)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='my_model')\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "272/272 [==============================] - 87s 294ms/step - loss: 0.5648 - accuracy: 0.7219 - val_loss: 0.5405 - val_accuracy: 0.8150\n",
      "Epoch 2/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.5461 - accuracy: 0.7242 - val_loss: 0.4960 - val_accuracy: 0.8147\n",
      "Epoch 3/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.5318 - accuracy: 0.7312 - val_loss: 0.5293 - val_accuracy: 0.8150\n",
      "Epoch 4/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.5313 - accuracy: 0.7323 - val_loss: 0.5337 - val_accuracy: 0.8150\n",
      "Epoch 5/20\n",
      "272/272 [==============================] - 2s 8ms/step - loss: 0.5096 - accuracy: 0.7440 - val_loss: 0.5568 - val_accuracy: 0.8147\n",
      "Epoch 6/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.4993 - accuracy: 0.7510 - val_loss: 0.5628 - val_accuracy: 0.8140\n",
      "Epoch 7/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.4920 - accuracy: 0.7576 - val_loss: 0.5297 - val_accuracy: 0.8134\n",
      "Epoch 8/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.4762 - accuracy: 0.7662 - val_loss: 0.5247 - val_accuracy: 0.8134\n",
      "Epoch 9/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.4554 - accuracy: 0.7809 - val_loss: 0.5650 - val_accuracy: 0.8131\n",
      "Epoch 10/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.4421 - accuracy: 0.7892 - val_loss: 0.5588 - val_accuracy: 0.8115\n",
      "Epoch 11/20\n",
      "272/272 [==============================] - 2s 8ms/step - loss: 0.4188 - accuracy: 0.8036 - val_loss: 0.6138 - val_accuracy: 0.8124\n",
      "Epoch 12/20\n",
      "272/272 [==============================] - 2s 8ms/step - loss: 0.4010 - accuracy: 0.8129 - val_loss: 0.6159 - val_accuracy: 0.8124\n",
      "Epoch 13/20\n",
      "272/272 [==============================] - 2s 8ms/step - loss: 0.3763 - accuracy: 0.8303 - val_loss: 0.5804 - val_accuracy: 0.7992\n",
      "Epoch 14/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.3494 - accuracy: 0.8463 - val_loss: 0.6371 - val_accuracy: 0.8031\n",
      "Epoch 15/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.3239 - accuracy: 0.8596 - val_loss: 0.6848 - val_accuracy: 0.8057\n",
      "Epoch 16/20\n",
      "272/272 [==============================] - 2s 8ms/step - loss: 0.3032 - accuracy: 0.8718 - val_loss: 0.6755 - val_accuracy: 0.8079\n",
      "Epoch 17/20\n",
      "272/272 [==============================] - 2s 8ms/step - loss: 0.2782 - accuracy: 0.8871 - val_loss: 0.6678 - val_accuracy: 0.7934\n",
      "Epoch 18/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.2568 - accuracy: 0.8977 - val_loss: 0.7125 - val_accuracy: 0.8050\n",
      "Epoch 19/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.2286 - accuracy: 0.9130 - val_loss: 0.7801 - val_accuracy: 0.7986\n",
      "Epoch 20/20\n",
      "272/272 [==============================] - 2s 7ms/step - loss: 0.2109 - accuracy: 0.9235 - val_loss: 0.7702 - val_accuracy: 0.7995\n"
     ]
    }
   ],
   "source": [
    "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)\n",
    "                                            \n",
    "history = my_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, axis=0, **kwargs):\n",
    "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
    "    self.axis = axis\n",
    "\n",
    "  def call(self, input):\n",
    "    return tf.math.reduce_mean(input, axis=self.axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./stuttering_yamnet/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./stuttering_yamnet/assets\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = './stuttering_yamnet'\n",
    "\n",
    "input_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\n",
    "embedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n",
    "                                            trainable=False, name='yamnet')\n",
    "_, embeddings_output, _ = embedding_extraction_layer(input_segment)\n",
    "serving_outputs = my_model(embeddings_output)\n",
    "serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
    "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
    "serving_model.save(saved_model_path, include_optimizer=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load('./stuttering_yamnet')\n",
    "\n",
    "testing_wav_data = load_wav_16k_mono(\"Downloads/ml-stuttering-events-dataset/clips/FluencyBank/093/FluencyBank_093_\"+str(1)+\".wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main sound is: Speech\n"
     ]
    }
   ],
   "source": [
    "reloaded_results = reloaded_model(testing_wav_data)\n",
    "stutter = my_classes[tf.argmax(reloaded_results)]\n",
    "print(f'The main sound is: {stutter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('vw': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6ba591e368c3cae059132ed7a03fe1a0cef1b596efa79ada7593eaf08b675f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
